---
title: "HW_1_Yeonji_Frankie_Ofer"
output:
  pdf_document:
    latex_engine: xelatex
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Dataset Description 1

I chose to describe the [“2005-2010 Graduation Outcomes - School Level”](https://data.cityofnewyork.us/Education/2005-2010-Graduation-Outcomes-School-Level/vh2h-md7a) dataset from NYCOpenData website 

Motivation

This dataset was created by the Department of Education (DOE) for the purpose of tracking year-over-year trends in student graduation outcomes within the New York City (NYC) school system. This dataset could be used to better understand the performance of specific schools or school district within various demographics in order to inform policy making and resource allocation. It can also be accessed and used by parents in the process of choosing a school for their daughter or son. The creation of the dataset was funded by the DOE.

Data Composition

The data is mostly numerical with some textual columns. Each row within the dataset is per demographic (total cohort, special ed, general education, asian, black, hispanic, white, english learners, english proficient, male, female) school per year between the cohort years of 2001-2006 (graduation years of 2005-2010) with a duplicate row for each 2006 cohort (appearing as “2006 Aug”). The relationship between the different rows occurs if they belong to the same school and have a population overlap (e.g. a hispanic female student who is proficient in english will appear in 4 different rows).
The dataset consists of 25,096 unique instances, each is a demographic-school-cohort. There are missing rows in the dataset. The dataset does not rely on any external sources and does not include any recommendations for analysis.

Data Collection Process

The data collected are the number of graduates within each school with either a Local or Regents diploma along with demographics data and enrollment status (dropped out/still enrolled).The data is available to the DOE and was internally processed, association between instances and individual level data was most likely also done by the DOE. The data was collected over the course of 6 years (2005-2010). 

The dataset does not contain all possible instances. Some schools will not have all different populations every year. In addition, Records with cohorts of 20 students or less were suppressed to protect any privacy violations. Also, 79 schools were not presented individually but were included in the city wide calculations.



Data Processing

Aside from the aforementioned data manipulations no additional data manipulations were reported. The raw data is private and therefore not publicly available. The data as provided in NYC Open Data could achieve its intended goal yet there are numerous more related research questions it could support better (for example, school budgets, parent education, socio-economic statuses, etc.)

Dataset Distribution

The dataset can be downloaded from NYC Open Data as a csv file (https://data.cityofnewyork.us/Education/2005-2010-Graduation-Outcomes-School-Level/vh2h-md7a). NYC Open Data portal is currently the owner of the dataset which was released publicly on October 18, 2011. There seem to be no restrictions over the data usage or any licensing requirements.

Dataset Maintenance 

NYC Open Data is the current owner of the dataset and has last updated it on September 10, 2018. There is no linked repository for this dataset or any way to contribute to it.

## Dataset Description 2

The dataset I chose is the studentVLE dataset from  [“Open University Learning Analytics Dataset (OULAD)”](https://analyse.kmi.open.ac.uk/open_dataset). The OULAD contains 7 different csv files about courses, student demographics and registration, student assessment results, and student interactions with Virtual Learning Environment (VLE) for 22 courses (named module) taught at the Open University (OU) during 2013 and 2014. Among them, I chose student VLE dataset to specifically examine student online learning behaviors with VLE during the term.
 
Motivation

This dataset was collected by the OU for the purpose of examining student learning experiences with interactions in their Virtual Learning Environment (VLE) system. The potential use of this data set involves examining student learning behaviors represented by their click actions in VLE. This dataset could be also used to understand how much and long do students interact with the specific course material. Based on this analysis, this dataset could be used to improve their course design, focusing on how to design the specific course activities in VLE considering the various demographics. Papers or examples using this dataset is listed in https://analyse.kmi.open.ac.uk/open_dataset.
 
Data Composition

The dataset consists of 10,655,280 observations with the total six columns with numerical and categorical data. Each row shows the information about each student’s interactions with the specific material in the VLE for certain course. Each column indicates the name (i.e. identification code) of course module, course presentation, student id, VLE material, the date of how long students continued interacting with the material in VLE since the start of the module-presentation, and the number of clicks students made with the material in the specific date. There is some relationship between the different rows if a student belongs to the same course module, same course presentation, and VLE material (e.g. a student identified 28300 who is interacting with the 546652 VLE material on the 2013J presentation in the AAA course module appears in 3 different rows). This dataset has 32,593 unique students interacting with the VLE within 22 courses while there are 10,655,280 entries as a whole. The dataset relies on other external sources of the other csv files in the OULAD, including more information about the students and courses (e.g. adding the studentInfo csv file to the dataset could be helpful to examine the relationships between student interaction activity and their own demographic information).
 
Data Collection & Processing Process
 
This dataset involved some researchers in Knowledge Media Institute at the OU in the data collection and processing process who hoped to support the research in field of learning analytics and enhance learning experiences. The dataset was extracted in a data warehouse in the OU from 2013 to 2014, which gathers information from all available information systems using SAS technology. Then, the subset of the dataset was anonymized as some data (e.g. course module, student id) contained identifiable, personal information. The name of course module, course presentation, and student VLE material were completely randomized and replaced by numeric or alphabetical identifiers (e.g. 587283, AAA). IN the data anonymization, they used used the ARX anonymization tool. Then, the dataset was created as a subset of separate csv files in the whole dataset structure. The whole process of data collection and processing was under the ethical and privacy requirements applied at the Open University. The dataset does not include all possible instances. This dataset is a subset of the OU student data from 2013 and 2014, which does not contain the other course modules that the students also might take and interact with the VLE material.
 
Data Distribution
 
The dataset can be downloaded from [OULAD](https://analyse.kmi.open.ac.uk/open_dataset) website with a DOI named https://doi.org/10.6084/m9.figshare.5081998.v1 as a csv file. This dataset was released under CC-BY 4.0 license in June 2017. Those who downloaded the dataset would be free to copy and redistribute the material and transform the material for any purpose even commercially. Thus there seems to be no restrictions over the data use.
 
Data Maintenance
 
The dataset is managed by the administers named Jakub Kuzilek, Martin Hlosta and Zdenek Zdrahal in Knowledge Media Institute at the OU. They are in charge of all comments and feedbacks related to the whole set of OULAD. There is no information whether this data has been updated after their release in June 2017. There is a reference for whom want to cite this dataset: Kuzilek J., Hlosta M., Zdrahal Z. [Open University Learning Analytics dataset](https://www.nature.com/articles/sdata2017171)Sci. Data 4:170171 doi: 10.1038/sdata.2017.171 (2017).


## Dataset Description 3

The dataset describes below is from the [City of Los Angeles MyLA311 Service Request Data 2018](https://data.lacity.org/A-Well-Run-City/MyLA311-Service-Request-Data-2018/h65r-yf5i).

Motivation 

The dataset is created by City of Los Angeles Information Technology Agency. It allows city officials or residents to gain a better understanding of the nature of the 311 call requests and geographic locations of these requests. It also provides insights on how these requests are being handled and their progress. For the Information Technology Agency, it allows them to understanding the usage behavior of their target users. Trends and patterns can be observed by comparing dataset across different years. Viewers could potentially be tempted to take these requests and issues into their own hands.

Dataset Composition

The dataset is composed of rows and columns of numeric, textual and categorical data. Each row represents a service request call includes information from created date, updated date, action taken, owner, request type, status, request source, address with a total of 33 variables. Each row represents an isolated service request. The dataset as of Sep 24 2018 consists of 1,154,435 records. There are missing data in a number of columns in some of the records.

Data Collection Process

The data is collected both manually and through software interface since users can submit 311 service request via a variety of channels includes call center, email, mobile apps, websites and other resources. LA 311 service officers are responsible for curating the service requests. This particular dataset is related to service requests submitted in 2018. Each data instance is independent of each other. There are instances that the same service request at the same location are being submitted by multiple users.This dataset contains all possible instances. There is missing information in the dataset. It seems that the reason that some of the information are missing is because they aren’t recorded while some other missing information are not entered into the dataset. There are several address related variables are redundant. The values of these variables could be easily extracted from the values in the address variable. 

Data Preprocessing

Preprocessing and on-going processing and update of the data is being done by the LA 311 service officers. The raw data of service requests submitted through 311 call centers and other resources is not available to the public. This dataset has achieved its intended goal for allowing different stakeholders to gain a better understanding of the 311 service requests.

Dataset Distribution

This dataset is distributed through Los Angeles Open Data portal. It can be accessed through SODA API and OData. It also supports various formats for downloading includes CSV, JSON, RDF, RSS, XML and other formats. This dataset is first available on Jan 1 2018. It’s distributed under [Creative Commons license zero (CC0)](https://creativecommons.org/publicdomain/zero/1.0/legalcode). 


Dataset Maintenance

This dataset is hosted by the City of Los Angeles on its open data portal. The owner is the Information Technology department which can be contacted on the portal. The dataset is scheduled to be updated daily throughout 2018.





```{r}
#Loading relevant libraries
library(tidyverse)
library(lubridate)
library(ggplot2)

#Loading filtered data files
trips <- read_csv("trips_hw_1.csv")
fares <- read_csv("fares_hw_1.csv")

#Converting to tibbles
trips <- as.tibble(trips)
fares <- as.tibble(fares)
```

## Data Cleaning
### Trips tibble cleaning

```{r}
#Limiting passanger count to between 1 and 6
trips <- filter(trips, between(trips$passenger_count,1,6))
#Removing 0 value distances and limiting trip distance to 500 miles
trips <- filter(trips, between(trips$trip_distance,0,500))
#filtering trip time in secs between 60 seconds to 7200 seconds (2 hours)
trips <- filter(trips, between(trips$trip_time_in_secs,60,7200))
#checking trip time in seconds using the pickup and dropoff datetimes 
#removing all observations in which there is a difference of over 10 seconds
trips<- trips %>% 
  mutate(time_diff=as.duration(dropoff_datetime-pickup_datetime)) %>% 
  mutate(different=abs(time_diff-as.duration(trips$trip_time_in_secs))>as.duration(10)) %>% 
  filter(!different)
#limiting fare codes to 1-6
trips <- filter(trips, between(trips$rate_code,1,6))
#removing irrelevant columns
trips <- select(trips, -vendor_id,-store_and_fwd_flag, -c(11:16))
View(trips)
```

### Fares file cleaning
```{r}
#Only consider data with cash or credit card payment
fares <- filter(fares, payment_type %in% c('CRD', 'CSH'))
#Remove data with fare amount less than 0.5
fares <- filter(fares, fare_amount >= 0.5)
#Remove data which tip amount is more than 50% of fare amount
fares <- filter(fares, tip_amount/fare_amount < 0.5)
#Remove data with discrepancy between calculated total amount and total_amount
fares <- filter(fares, fare_amount+surcharge+mta_tax+tip_amount+tolls_amount==total_amount)
#Remove vendor_id column
fares <- select(fares, -vendor_id)
View(fares)
```


### Join Trips and Fares
```{r}
#Check for key for trips
trips %>% count(medallion, hack_license, pickup_datetime) %>% filter(n>1)
#Check for key for fares
fares %>% count(medallion, hack_license, pickup_datetime) %>% filter(n>1)
#check specific fare entry
fares %>% filter(medallion==2013000386,hack_license==2013000384,pickup_datetime==ymd_hms(20130815171800))

#Remove duplicate rows
distinct(trips)
distinct(fares)

#Only keep the rows with higher total_amount 
fares <- fares %>% arrange(desc(total_amount)) %>% distinct(medallion, hack_license, pickup_datetime, .keep_all = TRUE)

#Join trips and fares
taxi_data <- inner_join(trips,fares)
```

# Part E
```{r}
# i) Add column for total trips
medallion_data <- taxi_data %>% group_by(medallion) %>% mutate(total_trips = n())

# ii) Add column for total passengers
medallion_data <- medallion_data %>% group_by(medallion) %>% mutate(total_passengers = sum(passenger_count))

# iii) Add column for total time with passengers
medallion_data <- medallion_data %>% group_by(medallion) %>% mutate(total_time_with_passengers = sum(trip_time_in_secs))

# iii) Add column for total time with passengers with 8/16 cutoff
cutoff = ymd_hms("2013-08-16 00:00:00")

medallion_data <- medallion_data %>% mutate(trip_time_on_next_day = ifelse( as.numeric(difftime(cutoff,dropoff_datetime,units="secs"))<0,as.numeric(difftime(cutoff,dropoff_datetime,units="secs")),0)) %>% group_by(medallion) %>% mutate(total_time_with_passengers_2 = sum(trip_time_in_secs+trip_time_on_next_day))

# iv) Add column for total distance traveled
medallion_data <- medallion_data %>% group_by(medallion) %>% mutate(total_distance = sum(trip_distance))

# v) Add column for total earnings
medallion_data <- medallion_data %>% group_by(medallion) %>% mutate(total_earnings = sum(total_amount))

# Select relevant columns for the final output
partE_temp <- select(medallion_data, c(medallion, total_trips, total_passengers, total_time_with_passengers, total_distance, total_earnings))

# Remove the duplicated rows; Make a list of taxicab
partE_output <- partE_temp %>% distinct(partE_temp, medallion, .keep_all=T)

```
# Part F
```{r}

# Add column for hour 
add_hour <- medallion_data %>% mutate(hours=format(as.POSIXct(pickup_datetime, format="%Y-%m-%d %H:%M"), format="%H") )

# i) Add column for total passengers picked up
partFi <- add_hour %>% group_by(hack_license, hours) %>%  mutate(total_passengers_picked_up = sum(passenger_count))

# ii) Add column for trips started
partFii <- partFi %>% group_by(hack_license, hours) %>%  mutate(trips_started = n())

# Select relevant columns for the final output
partF_temp <- select(partFii, c(hack_license, hours, total_passengers_picked_up, trips_started))

# Remove the duplicated rows; Make a list of taxicab
partF_output <- partF_temp %>% distinct(partF_temp, hack_license, .keep_all=T)

```


# Part G
```{r}

taxi_data <- taxi_data %>% 
  #Extracting hours
  mutate(pickup_hour_time = as.integer(format(pickup_datetime,format='%H'))) %>%
  mutate(dropoff_hour_time = as.integer(format(dropoff_datetime,format='%H'))) %>%
  #Boolean for same hour rides and creating two columns for later use of adjusted pickup and dropoff times
  mutate(adjusted_pickup_time=pickup_datetime) %>% 
  mutate(adjusted_dropoff_time=dropoff_datetime) 

# Separating to three tables, one with same hour rides, a second with rides that span two hours of the day and a 
# third with rides spanning 3 hours of the day, manually adding trips that end the next day
within_hour_rides <-filter(taxi_data, pickup_hour_time==dropoff_hour_time)
one_hour_diff <- rbind(filter(taxi_data, dropoff_hour_time==pickup_hour_time+1), 
                       filter(taxi_data, pickup_hour_time==23 & dropoff_hour_time==0)) 
two_hour_diff <- rbind(filter(taxi_data, dropoff_hour_time==pickup_hour_time+2),
                       filter(taxi_data, pickup_hour_time==22 & dropoff_hour_time==0),
                       filter(taxi_data, pickup_hour_time==23 & dropoff_hour_time==1))  
#Duplicating each row in the drives that span 2 hours of the day and adjusting the pickup and dropoff times for those
one_hour_diff_dup <- rbind(one_hour_diff %>% 
               mutate(adjusted_dropoff_time=ceiling_date(pickup_datetime, unit = "hours",change_on_boundary = FALSE)),
      one_hour_diff %>% 
        mutate(adjusted_pickup_time=floor_date(dropoff_datetime, unit = "hours")))

#Duplicating each row in the drives that span 3 hours of the day and adjusting the pickup and dropoff times for those

two_hour_diff_dup <- rbind(two_hour_diff %>% 
        mutate(adjusted_dropoff_time=ceiling_date(pickup_datetime, unit = "hours",change_on_boundary = FALSE)),
      two_hour_diff %>% 
        mutate(adjusted_pickup_time=floor_date(dropoff_datetime, unit = "hours")),
      two_hour_diff %>% 
        mutate(adjusted_dropoff_time=floor_date(dropoff_datetime, unit = "hours")) %>% 
        mutate(adjusted_pickup_time=ceiling_date(pickup_datetime, unit = "hours",change_on_boundary = FALSE)))

# Spot checking shows that for every rides spanning two hours we now have two rides, one ending at the turn of the hour and another starting at the turn of the hour. Same applies for rides spanning more than 2 hours of the day where the "middle" ride is a full hour ride.
# Combining all three tables and removing "rides" that have an adjusted start time on the next day
Duplicated_rides <- rbind(within_hour_rides,one_hour_diff_dup,two_hour_diff_dup)
Duplicated_rides <- filter(Duplicated_rides, format(adjusted_pickup_time,format='%Y:%m:%d')!='2013-08-15')                   
# Creating the relevant variables
Duplicated_rides <- Duplicated_rides %>% 
  mutate(adj_trip_time_in_secs = as.duration(adjusted_dropoff_time-adjusted_pickup_time)) %>% 
  mutate(average_speed = as.numeric(trip_distance/trip_time_in_secs)) %>% 
  mutate(adj_trip_dist = as.numeric(average_speed*adj_trip_time_in_secs)) %>% 
  mutate(adj_total_fare = as.numeric(total_amount*adj_trip_time_in_secs/trip_time_in_secs)) %>% 
  mutate(adj_pickup_hour = as.integer(format(pickup_datetime,format='%H')))

#Grouping and summarising the requested results
partG_output <- Duplicated_rides %>% 
  group_by(hack_license,adj_pickup_hour) %>% 
  summarise(sum(adj_trip_time_in_secs),sum(adj_trip_dist),sum(adj_total_fare))

names(partG_output) <- c("hack_license","hour","total_time_with_passengers","miles_with_passengers","earnings")
View(partG_output)

```


## Plotting 1

```{r}

#Creating a table grouped by hour of day for analysis
Per_hour_stats <- partG_output %>% group_by(hour) %>% summarise(mean(total_time_with_passengers), mean(miles_with_passengers), mean(earnings),n())
names(Per_hour_stats) <- c("hour","avg_time_with_passenegers","avg_miles_with_passenegers","avg_earnings","total_rides")
#creating two new variables for analysis
Per_hour_stats <- mutate(Per_hour_stats,avg_per_mile_earnings = avg_earnings/avg_miles_with_passenegers)
Per_hour_stats <- mutate(Per_hour_stats,avg_per_second_earnings = avg_earnings/avg_time_with_passenegers)

#Plotting the hour of day stats
ggplot(data = Per_hour_stats) + 
  geom_point(mapping = aes(x = hour, y = total_rides))

ggplot(data = Per_hour_stats) + 
  geom_point(mapping = aes(x = hour, y = avg_per_mile_earnings))

ggplot(data = Per_hour_stats) + 
  geom_point(mapping = aes(x = hour, y = avg_per_second_earnings))

```

### Plotting 1 Analysis
We created three scatter plots to analyse the rides patterns according to hour of the day on that specific date. We found that when looking at total number of rides per hour we see a rather steady high demand during the daytime between the hours of 8am and 2 pm, followed by a small decrease in demand for rides until 5pm. The demand for rides then climbs back up to reach a maximum of almost 12,000 at 7pm and steadily decreases until midnight. The demand for rides 2am and 5am is significantly lower as to be expected.

When looking at average per mile earnings we find that at 7pm a cab driver (on that day) will make on average about 6 dollars per mile (makes you think twice before taking a cab on that hour), while the minimum is at 5am in which traffic is virtually non-existent and the average per mile earning drops to about 4 dollars.

Contrasted with average earnings per second spent with passengers, we see that 5am is actually the maximum with earnings of about 3 cents per second spent with passengers while 7pm which is the most lucrative hour per mile is somewhere between 1.5 cents per second and 2 cents per second.


## Plotting 2

```{r}
#Making plots with the frequency for (a) total earnings and (b) total trips, total distance, total time.

ggplot(partE_output, aes(x = total_earnings, y = total_trips)) +
  geom_bin2d(bins = 40)

ggplot(partE_output, aes(x = total_earnings, y = total_time_with_passengers)) +
  geom_bin2d(bins = 40)

ggplot(partE_output, aes(x = total_earnings, y = total_distance)) +
  geom_bin2d(bins = 40)
```

### Plotting 2 Analysis
We also tried to just check whether total earnings actually relate to total trips, distance, and time. We created three scatter plots with geom_bin2d function by adding a heat map of 2d bin counts, which is easier for us to see the relationship considering the frequency. 

When looking at the total trips and earnings together, we found that getting more money naturally related to the number of trips the drivers had on that day. The plot showed a positive correlation between two variables. We were also able to check the distribution of both earnings and trips on that day, showing that there were many drivers who got around $500 to $625 with around 40 trips on that day. 

Similar as the prior plot, the plot of total earnings and times showed a positive relationship. It showed that as the driver got more money on that day, they spent more time on carrying passengers. For the distribution of both earnings and times on that day, it seemed that there were many drivers who earned around $500 and $700 by spending around 30,000 seconds carrying passengers on that day.

When examining the relationship between total earnings and distance, however, it seems that those two variables were not much related to each other. Because of some outliers, the plot had a kind of maximum of y axis, not showing clear patterns of their relationships b. Across the total earnings the drivers made on that day, the total distance varied a lot as shown as a “line” of the relationship between two variables. With the further assignment of the limit to the maximum value of y axis or deleting the outliers, the plot would show more clear pattern of the relationship. 

## Plotting 3

```{r}

ggplot(data = Per_hour_stats) + geom_point(mapping = aes(x = hour, y=avg_time_with_passenegers))

ggplot(data = Per_hour_stats) + geom_point(mapping = aes(x = hour, y=avg_earnings))

ggplot(data = partG_output) + geom_point(mapping = aes(x = total_time_with_passengers, y=earnings)) + geom_smooth(aes(x =total_time_with_passengers, y = earnings)) + facet_wrap(~hour, nrow = 4)

```

### Plotting 3 Analysis

We have shown there is a positive correlation between time with passengers and earnings from the plot above. When we further examine relationship between average timewith passengers and hour of day, we notice there is a sharp increase in time with passengers from 5am to 8am. Time  with passengers also gradually increases from 10am to 6pm to reach its highest point at around 2349 seconds. It then decreases significant from 6pm to 5am until it reaches its lowest point at around 1039 seconds. Whe we compare this plot with the plot using hour of day and average earnings, it reveals a similar pattern as hour of day and time with passengers except the discrepency between 9pm to 11pm and 3am to 5am where time with passengers decreases while earnings increases in that period. Further investigation is required to reveal the cause of these discrepencies.

We then examine the relationship between time with passengers, hour of day and earnings from Part G output. We notice the plots of most of the hours suggest a linear relationship between time with passengers and earnings. However, some plots  (for instance 12am, 3am and 1pm) reveal the existance of potential outliers that requires our further investigation. 



